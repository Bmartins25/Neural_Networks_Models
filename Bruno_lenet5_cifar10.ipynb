{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bmartins25/Redes_Neurais_Modelos/blob/main/Bruno_lenet5_cifar10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkoGLH_Tj5wn"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#pip install watermark"
      ],
      "metadata": {
        "id": "oBu6N2vJmUw_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext watermark\n",
        "%watermark -a 'Bruno Bartolomeu' -v -p torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "urQooFhXmFyR",
        "outputId": "3a09e93b-d862-4310-e35c-406ee5081440"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The watermark extension is already loaded. To reload it, use:\n",
            "  %reload_ext watermark\n",
            "Author: Bruno Bartolomeu\n",
            "\n",
            "Python implementation: CPython\n",
            "Python version       : 3.10.12\n",
            "IPython version      : 7.34.0\n",
            "\n",
            "torch: 2.0.1+cu118\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ORj09gnrj5wp"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6hghKPxj5w0"
      },
      "source": [
        "## Model Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "NnT0sZIwj5wu"
      },
      "outputs": [],
      "source": [
        "##########################\n",
        "### SETTINGS\n",
        "##########################\n",
        "\n",
        "# Hyperparameters\n",
        "RANDOM_SEED = 1\n",
        "LEARNING_RATE = 0.001\n",
        "BATCH_SIZE = 128\n",
        "NUM_EPOCHS = 10\n",
        "\n",
        "# Architecture\n",
        "NUM_FEATURES = 32*32\n",
        "NUM_CLASSES = 10\n",
        "\n",
        "# Other\n",
        "DEVICE = \"cuda:0\"\n",
        "GRAYSCALE = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYEgPorhj0B_"
      },
      "source": [
        "### MNIST Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BG22BhxYj0CA",
        "outputId": "c2063bc1-0ea9-4d82-943e-af6dade39717"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:05<00:00, 29750924.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/cifar-10-python.tar.gz to data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image batch dimensions: torch.Size([128, 3, 32, 32])\n",
            "Image label dimensions: torch.Size([128])\n",
            "Image batch dimensions: torch.Size([128, 3, 32, 32])\n",
            "Image label dimensions: torch.Size([128])\n"
          ]
        }
      ],
      "source": [
        "##########################\n",
        "### CIFAR-10 Dataset\n",
        "##########################\n",
        "\n",
        "\n",
        "# Note transforms.ToTensor() scales input images\n",
        "# to 0-1 range\n",
        "train_dataset = datasets.CIFAR10(root='data',\n",
        "                                 train=True,\n",
        "                                 transform=transforms.ToTensor(),\n",
        "                                 download=True)\n",
        "\n",
        "test_dataset = datasets.CIFAR10(root='data',\n",
        "                                train=False,\n",
        "                                transform=transforms.ToTensor())\n",
        "\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset,\n",
        "                          batch_size=BATCH_SIZE,\n",
        "                          num_workers=8,\n",
        "                          shuffle=True)\n",
        "\n",
        "test_loader = DataLoader(dataset=test_dataset,\n",
        "                         batch_size=BATCH_SIZE,\n",
        "                         num_workers=8,\n",
        "                         shuffle=False)\n",
        "\n",
        "# Checking the dataset\n",
        "for images, labels in train_loader:\n",
        "    print('Image batch dimensions:', images.shape)\n",
        "    print('Image label dimensions:', labels.shape)\n",
        "    break\n",
        "\n",
        "# Checking the dataset\n",
        "for images, labels in train_loader:\n",
        "    print('Image batch dimensions:', images.shape)\n",
        "    print('Image label dimensions:', labels.shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WT0T8qtDj0CC",
        "outputId": "de23f04c-02e4-4954-9822-8278470ea6bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 | Batch index: 0 | Batch size: 128\n",
            "Epoch: 2 | Batch index: 0 | Batch size: 128\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(DEVICE)\n",
        "torch.manual_seed(0)\n",
        "\n",
        "for epoch in range(2):\n",
        "\n",
        "    for batch_idx, (x, y) in enumerate(train_loader):\n",
        "\n",
        "        print('Epoch:', epoch+1, end='')\n",
        "        print(' | Batch index:', batch_idx, end='')\n",
        "        print(' | Batch size:', y.size()[0])\n",
        "\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "dtxUaH82j0CD"
      },
      "outputs": [],
      "source": [
        "##########################\n",
        "### MODEL\n",
        "##########################\n",
        "\n",
        "\n",
        "class LeNet5(nn.Module):\n",
        "\n",
        "    def __init__(self, num_classes, grayscale=False):\n",
        "        super(LeNet5, self).__init__()\n",
        "\n",
        "        self.grayscale = grayscale\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        if self.grayscale:\n",
        "            in_channels = 1\n",
        "        else:\n",
        "            in_channels = 3\n",
        "\n",
        "        self.features = nn.Sequential(\n",
        "\n",
        "            nn.Conv2d(in_channels, 6*in_channels, kernel_size=5),\n",
        "            nn.Tanh(),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            nn.Conv2d(6*in_channels, 16*in_channels, kernel_size=5),\n",
        "            nn.Tanh(),\n",
        "            nn.MaxPool2d(kernel_size=2)\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(16*5*5*in_channels, 120*in_channels),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(120*in_channels, 84*in_channels),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(84*in_channels, num_classes),\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        logits = self.classifier(x)\n",
        "        probas = F.softmax(logits, dim=1)\n",
        "        return logits, probas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "_lza9t_uj5w1"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(RANDOM_SEED)\n",
        "\n",
        "model = LeNet5(NUM_CLASSES, GRAYSCALE)\n",
        "model.to(DEVICE)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAodboScj5w6"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dzh3ROmRj5w7",
        "outputId": "6240607b-9fe4-4a42-e50a-d04dafb636ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001/010 | Batch 0000/0391 | Cost: 2.3115\n",
            "Epoch: 001/010 | Batch 0050/0391 | Cost: 1.7791\n",
            "Epoch: 001/010 | Batch 0100/0391 | Cost: 1.5885\n",
            "Epoch: 001/010 | Batch 0150/0391 | Cost: 1.5896\n",
            "Epoch: 001/010 | Batch 0200/0391 | Cost: 1.3669\n",
            "Epoch: 001/010 | Batch 0250/0391 | Cost: 1.4077\n",
            "Epoch: 001/010 | Batch 0300/0391 | Cost: 1.2788\n",
            "Epoch: 001/010 | Batch 0350/0391 | Cost: 1.6199\n",
            "Epoch: 001/010 | Train: 53.018%\n",
            "Time elapsed: 0.41 min\n",
            "Epoch: 002/010 | Batch 0000/0391 | Cost: 1.4412\n",
            "Epoch: 002/010 | Batch 0050/0391 | Cost: 1.1881\n",
            "Epoch: 002/010 | Batch 0100/0391 | Cost: 1.3512\n",
            "Epoch: 002/010 | Batch 0150/0391 | Cost: 1.1598\n",
            "Epoch: 002/010 | Batch 0200/0391 | Cost: 1.2648\n",
            "Epoch: 002/010 | Batch 0250/0391 | Cost: 1.0852\n",
            "Epoch: 002/010 | Batch 0300/0391 | Cost: 1.1316\n",
            "Epoch: 002/010 | Batch 0350/0391 | Cost: 1.2017\n",
            "Epoch: 002/010 | Train: 61.578%\n",
            "Time elapsed: 0.69 min\n",
            "Epoch: 003/010 | Batch 0000/0391 | Cost: 1.1980\n",
            "Epoch: 003/010 | Batch 0050/0391 | Cost: 1.1403\n",
            "Epoch: 003/010 | Batch 0100/0391 | Cost: 1.0798\n",
            "Epoch: 003/010 | Batch 0150/0391 | Cost: 1.0367\n",
            "Epoch: 003/010 | Batch 0200/0391 | Cost: 1.1599\n",
            "Epoch: 003/010 | Batch 0250/0391 | Cost: 1.2319\n",
            "Epoch: 003/010 | Batch 0300/0391 | Cost: 1.2141\n",
            "Epoch: 003/010 | Batch 0350/0391 | Cost: 1.0619\n",
            "Epoch: 003/010 | Train: 65.810%\n",
            "Time elapsed: 0.98 min\n",
            "Epoch: 004/010 | Batch 0000/0391 | Cost: 0.9994\n",
            "Epoch: 004/010 | Batch 0050/0391 | Cost: 1.0912\n",
            "Epoch: 004/010 | Batch 0100/0391 | Cost: 1.0047\n",
            "Epoch: 004/010 | Batch 0150/0391 | Cost: 0.9850\n",
            "Epoch: 004/010 | Batch 0200/0391 | Cost: 1.1668\n",
            "Epoch: 004/010 | Batch 0250/0391 | Cost: 1.0121\n",
            "Epoch: 004/010 | Batch 0300/0391 | Cost: 1.2059\n",
            "Epoch: 004/010 | Batch 0350/0391 | Cost: 0.9884\n",
            "Epoch: 004/010 | Train: 69.006%\n",
            "Time elapsed: 1.28 min\n",
            "Epoch: 005/010 | Batch 0000/0391 | Cost: 0.9064\n",
            "Epoch: 005/010 | Batch 0050/0391 | Cost: 1.0367\n",
            "Epoch: 005/010 | Batch 0100/0391 | Cost: 0.7675\n",
            "Epoch: 005/010 | Batch 0150/0391 | Cost: 0.8684\n",
            "Epoch: 005/010 | Batch 0200/0391 | Cost: 0.9201\n",
            "Epoch: 005/010 | Batch 0250/0391 | Cost: 0.8571\n",
            "Epoch: 005/010 | Batch 0300/0391 | Cost: 1.0137\n",
            "Epoch: 005/010 | Batch 0350/0391 | Cost: 0.7679\n",
            "Epoch: 005/010 | Train: 70.782%\n",
            "Time elapsed: 1.55 min\n",
            "Epoch: 006/010 | Batch 0000/0391 | Cost: 0.7918\n",
            "Epoch: 006/010 | Batch 0050/0391 | Cost: 0.8539\n",
            "Epoch: 006/010 | Batch 0100/0391 | Cost: 0.8074\n",
            "Epoch: 006/010 | Batch 0150/0391 | Cost: 0.8332\n",
            "Epoch: 006/010 | Batch 0200/0391 | Cost: 0.8858\n",
            "Epoch: 006/010 | Batch 0250/0391 | Cost: 0.7433\n",
            "Epoch: 006/010 | Batch 0300/0391 | Cost: 0.9780\n",
            "Epoch: 006/010 | Batch 0350/0391 | Cost: 0.9183\n",
            "Epoch: 006/010 | Train: 75.964%\n",
            "Time elapsed: 1.84 min\n",
            "Epoch: 007/010 | Batch 0000/0391 | Cost: 0.7317\n",
            "Epoch: 007/010 | Batch 0050/0391 | Cost: 0.8760\n",
            "Epoch: 007/010 | Batch 0100/0391 | Cost: 0.6979\n",
            "Epoch: 007/010 | Batch 0150/0391 | Cost: 0.6596\n",
            "Epoch: 007/010 | Batch 0200/0391 | Cost: 0.7653\n",
            "Epoch: 007/010 | Batch 0250/0391 | Cost: 0.8228\n",
            "Epoch: 007/010 | Batch 0300/0391 | Cost: 0.9133\n",
            "Epoch: 007/010 | Batch 0350/0391 | Cost: 0.8753\n",
            "Epoch: 007/010 | Train: 78.026%\n",
            "Time elapsed: 2.12 min\n",
            "Epoch: 008/010 | Batch 0000/0391 | Cost: 0.6025\n",
            "Epoch: 008/010 | Batch 0050/0391 | Cost: 0.6897\n",
            "Epoch: 008/010 | Batch 0100/0391 | Cost: 0.6133\n",
            "Epoch: 008/010 | Batch 0150/0391 | Cost: 0.5134\n",
            "Epoch: 008/010 | Batch 0200/0391 | Cost: 0.5355\n",
            "Epoch: 008/010 | Batch 0250/0391 | Cost: 0.8066\n",
            "Epoch: 008/010 | Batch 0300/0391 | Cost: 0.5418\n",
            "Epoch: 008/010 | Batch 0350/0391 | Cost: 0.6108\n",
            "Epoch: 008/010 | Train: 80.846%\n",
            "Time elapsed: 2.39 min\n",
            "Epoch: 009/010 | Batch 0000/0391 | Cost: 0.5986\n",
            "Epoch: 009/010 | Batch 0050/0391 | Cost: 0.4492\n",
            "Epoch: 009/010 | Batch 0100/0391 | Cost: 0.6194\n",
            "Epoch: 009/010 | Batch 0150/0391 | Cost: 0.6483\n",
            "Epoch: 009/010 | Batch 0200/0391 | Cost: 0.5281\n",
            "Epoch: 009/010 | Batch 0250/0391 | Cost: 0.5534\n",
            "Epoch: 009/010 | Batch 0300/0391 | Cost: 0.5325\n",
            "Epoch: 009/010 | Batch 0350/0391 | Cost: 0.6348\n",
            "Epoch: 009/010 | Train: 86.542%\n",
            "Time elapsed: 2.67 min\n",
            "Epoch: 010/010 | Batch 0000/0391 | Cost: 0.4274\n",
            "Epoch: 010/010 | Batch 0050/0391 | Cost: 0.4266\n",
            "Epoch: 010/010 | Batch 0100/0391 | Cost: 0.5012\n",
            "Epoch: 010/010 | Batch 0150/0391 | Cost: 0.5366\n",
            "Epoch: 010/010 | Batch 0200/0391 | Cost: 0.4049\n",
            "Epoch: 010/010 | Batch 0250/0391 | Cost: 0.6140\n",
            "Epoch: 010/010 | Batch 0300/0391 | Cost: 0.3649\n",
            "Epoch: 010/010 | Batch 0350/0391 | Cost: 0.5343\n",
            "Epoch: 010/010 | Train: 89.428%\n",
            "Time elapsed: 2.96 min\n",
            "Total Training Time: 2.96 min\n"
          ]
        }
      ],
      "source": [
        "def compute_accuracy(model, data_loader, device):\n",
        "    correct_pred, num_examples = 0, 0\n",
        "    for i, (features, targets) in enumerate(data_loader):\n",
        "\n",
        "        features = features.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        logits, probas = model(features)\n",
        "        _, predicted_labels = torch.max(probas, 1)\n",
        "        num_examples += targets.size(0)\n",
        "        correct_pred += (predicted_labels == targets).sum()\n",
        "    return correct_pred.float()/num_examples * 100\n",
        "\n",
        "\n",
        "start_time = time.time()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "\n",
        "    model.train()\n",
        "    for batch_idx, (features, targets) in enumerate(train_loader):\n",
        "\n",
        "        features = features.to(DEVICE)\n",
        "        targets = targets.to(DEVICE)\n",
        "\n",
        "        ### FORWARD AND BACK PROP\n",
        "        logits, probas = model(features)\n",
        "        cost = F.cross_entropy(logits, targets)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        cost.backward()\n",
        "\n",
        "        ### UPDATE MODEL PARAMETERS\n",
        "        optimizer.step()\n",
        "\n",
        "        ### LOGGING\n",
        "        if not batch_idx % 50:\n",
        "            print ('Epoch: %03d/%03d | Batch %04d/%04d | Cost: %.4f'\n",
        "                   %(epoch+1, NUM_EPOCHS, batch_idx,\n",
        "                     len(train_loader), cost))\n",
        "\n",
        "\n",
        "\n",
        "    model.eval()\n",
        "    with torch.set_grad_enabled(False): # save memory during inference\n",
        "        print('Epoch: %03d/%03d | Train: %.3f%%' % (\n",
        "              epoch+1, NUM_EPOCHS,\n",
        "              compute_accuracy(model, train_loader, device=DEVICE)))\n",
        "\n",
        "    print('Time elapsed: %.2f min' % ((time.time() - start_time)/60))\n",
        "\n",
        "print('Total Training Time: %.2f min' % ((time.time() - start_time)/60))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paaeEQHQj5xC"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gzQMWKq5j5xE",
        "outputId": "34d777a5-a0f1-49e9-8bcd-994c0133ab84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 66.75%\n"
          ]
        }
      ],
      "source": [
        "with torch.set_grad_enabled(False): # save memory during inference\n",
        "    print('Test accuracy: %.2f%%' % (compute_accuracy(model, test_loader, device=DEVICE)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fOA3yNy7j0CL",
        "outputId": "181d1c18-c81a-45e0-adaf-c311607bacc7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PIL        : 9.4.0\n",
            "pandas     : 1.5.3\n",
            "torchvision: 0.15.2+cu118\n",
            "matplotlib : 3.7.1\n",
            "numpy      : 1.23.5\n",
            "torch      : 2.0.1+cu118\n",
            "\n"
          ]
        }
      ],
      "source": [
        "%watermark -iv"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "toc": {
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": true,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "371px"
      },
      "toc_section_display": true,
      "toc_window_display": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}